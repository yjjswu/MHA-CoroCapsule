{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, Sequential, regularizers\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import math\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#import seaborn as sn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from evaluation import compute_performance_measures\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  #按需分配显存\n",
    "keras.backend.tensorflow_backend.set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "tf.random.set_seed(111111)\n",
    "np.random.seed(111111)\n",
    "random.seed(111111)\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    scale = K.sqrt(s_squared_norm) / (1 + s_squared_norm)\n",
    "    return scale * x\n",
    "\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "    return ex / K.sum(ex, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def margin_loss(y_true, y_pred):\n",
    "    lamb, margin = 0.5, 0.1\n",
    "    return K.sum((y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n",
    "        1 - y_true) * K.square(K.relu(y_pred - margin))), axis=-1)\n",
    "\n",
    "\n",
    "class Capsule(Layer):\n",
    "    def __init__(self,\n",
    "                 num_capsule,\n",
    "                 dim_capsule,\n",
    "                 routings=3,\n",
    "                 share_weights=True,\n",
    "                 activation='squash',\n",
    "                 **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'squash':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = activations.get(activation)\n",
    "            \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "        'num_capsule':  self.num_capsule,\n",
    "        'dim_capsule' : self.dim_capsule,\n",
    "        'routings':  self.routings,\n",
    "        'share_weight':self.share_weights,\n",
    "           \n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.kernel = self.add_weight(\n",
    "                name='capsule_kernel',\n",
    "                shape=(1, input_dim_capsule,\n",
    "                       self.num_capsule * self.dim_capsule),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.kernel = self.add_weight(\n",
    "                name='capsule_kernel',\n",
    "                shape=(input_num_capsule, input_dim_capsule,\n",
    "                       self.num_capsule * self.dim_capsule),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.share_weights:\n",
    "            hat_inputs = K.conv1d(inputs, self.kernel)\n",
    "        else:\n",
    "            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(inputs)[0]\n",
    "        input_num_capsule = K.shape(inputs)[1]\n",
    "        hat_inputs = K.reshape(hat_inputs,\n",
    "                               (batch_size, input_num_capsule,\n",
    "                                self.num_capsule, self.dim_capsule))\n",
    "        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n",
    "\n",
    "        b = K.zeros_like(hat_inputs[:, :, :, 0])\n",
    "        for i in range(self.routings):\n",
    "            c = softmax(b, 1)\n",
    "            o = self.activation(keras.backend.batch_dot(c, hat_inputs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = keras.backend.batch_dot(o, hat_inputs, [2, 3])\n",
    "                if K.backend() == 'theano':\n",
    "                    o = K.sum(o, axis=1)\n",
    "\n",
    "        return o\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(inputs):\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = AveragePooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = Reshape((-1, 128))(x)\n",
    "    x = Capsule(32, 8, 3, True)(x)  \n",
    "    x = Capsule(32, 8, 3, True)(x)   \n",
    "    capsule = Capsule(3, 16, 3, True)(x)\n",
    "    output = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\n",
    "    model = Model(inputs=[inputs], outputs=[output], name='COVID-CAPS')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"COVID-CAPS\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 126, 126, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 126, 126, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 124, 124, 64)      36928     \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 60, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 58, 58, 128)       147584    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 3364, 128)         0         \n",
      "_________________________________________________________________\n",
      "capsule (Capsule)            (None, 32, 8)             32768     \n",
      "_________________________________________________________________\n",
      "capsule_1 (Capsule)          (None, 32, 8)             2048      \n",
      "_________________________________________________________________\n",
      "capsule_2 (Capsule)          (None, 3, 16)             384       \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 295,616\n",
      "Trainable params: 295,488\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(128, 128, 3))\n",
    "model = create_model(inputs)\n",
    "adam = optimizers.Adam(lr=0.0001)     \n",
    "model.compile(loss=margin_loss, optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning decay rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.0001\n",
    "    drop = 0.5 \n",
    "    epochs_drop = 20  \n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  \n",
    "num_classes = 3\n",
    "epochs = 100\n",
    "\n",
    "images= np.load(\"data/image.npy\")\n",
    "labels= np.load(\"data/label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=3)\n",
    "cvscores = []\n",
    "cvpre = []\n",
    "cvrecall  =[]\n",
    "cvf1 = []\n",
    "cvauc = []\n",
    "\n",
    "for k, (train, test) in enumerate(kfold.split(images, labels)):\n",
    "    \n",
    "    path = \"model/\"\n",
    "    \n",
    "    x_train = images[train]\n",
    "    x_test = images[test]\n",
    "    y_train = labels[train]\n",
    "    y_test = labels[test]\n",
    "    \n",
    "#     np.save(path + 'data/x_train.npy', x_train)\n",
    "#     np.save(path + 'data/y_train.npy', y_train)\n",
    "#     np.save(path + 'data/x_test.npy', x_test)\n",
    "#     np.save(path + 'data/y_test.npy', y_test)\n",
    "    \n",
    "    #class weights to handle class imbalance\n",
    "    class_weights = {0: 1-np.count_nonzero(y_train==0)/len(y_train), \n",
    "                     1: 1-np.count_nonzero(y_train==1)/len(y_train), \n",
    "                     2: 1-np.count_nonzero(y_train==2)/len(y_train)}\n",
    "\n",
    "    # 将整型标签转为onehot\n",
    "    y_train = utils.to_categorical(y_train, num_classes)\n",
    "    y_test = utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    \n",
    "    # The best model is selected based on the loss value on the validation set\n",
    "    filepath = path+\"caps-weights-best.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "    # learning schedule callback\n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "    callbacks_list = [checkpoint, lrate]\n",
    "    \n",
    "\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    #===================================Model======================================\n",
    "    model = create_model(inputs)\n",
    "    adam = optimizers.Adam(lr=0.0001)     \n",
    "    model.compile(loss=margin_loss, optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    print(\"========================= 第\"+ str(k+1) +\"折开始 ============================\")\n",
    "    history = model.fit(x_train, y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    verbose=2,\n",
    "                    #validation_split=0.1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    class_weight=class_weights, \n",
    "                    shuffle=True, \n",
    "                    callbacks=callbacks_list)\n",
    "    \n",
    "#     np.save(path+'acc.npy', history.history['accuracy'])\n",
    "#     np.save(path+'val_acc.npy', history.history['val_accuracy'])\n",
    "#     np.save(path+'loss.npy', history.history['loss'])\n",
    "#     np.save(path+'val_loss.npy', history.history['val_loss'])\n",
    "    \n",
    "    model.load_weights(filepath)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "    \n",
    "    predict = model.predict([x_test])\n",
    "    \n",
    "    # ===================auc=========================\n",
    "    n_classes = y_test.shape[1]\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], predict[:, i], )\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    roc_auc[\"macro\"] = roc_auc_score(y_test, predict, multi_class=\"ovo\", average=\"macro\")\n",
    "    roc_auc[\"weighted\"] = roc_auc_score(y_test, predict, multi_class=\"ovo\", average=\"weighted\")\n",
    "    # ===================================================\n",
    "\n",
    "    y_pre = np.argmax(predict, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    report = classification_report(y_test, y_pre, output_dict=True)\n",
    "    df1 = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    df1['auc'] = [roc_auc[0], roc_auc[1], roc_auc[2], \" \", roc_auc[\"macro\"], roc_auc[\"weighted\"]]\n",
    "    \n",
    "    # Write it into csv format\n",
    "    #df1.to_csv(path+'report.csv', index=True, header=True)\n",
    "    \n",
    "    cvpre.append(df1.loc['macro avg','precision'] * 100)\n",
    "    cvrecall.append(df1.loc['macro avg','recall'] * 100)\n",
    "    cvf1.append(df1.loc['macro avg','f1-score'] * 100)\n",
    "    cvauc.append(df1.loc['macro avg','auc'] * 100)\n",
    "    \n",
    "#     data = confusion_matrix(y_test, y_pre)\n",
    "#     names = ['normal', 'pneumonia', 'COVID-19']\n",
    "#     df_cm = pd.DataFrame(data, columns=names, index=names)\n",
    "#     df_cm.to_csv(path+'cm.csv', index=True, header=True)\n",
    "    print(\"========================= 第\"+ str(k+1) +\"折结束 ============================\")\n",
    "    \n",
    "print(\"accuracy：%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "print(\"precision：%.2f%% (+/- %.2f%%)\" % (np.mean(cvpre), np.std(cvpre)))\n",
    "print(\"recall：%.2f%% (+/- %.2f%%)\" % (np.mean(cvrecall), np.std(cvrecall)))\n",
    "print(\"f1-score：%.2f%% (+/- %.2f%%)\" % (np.mean(cvf1), np.std(cvf1)))\n",
    "print(\"auc：%.2f%% (+/- %.2f%%)\" % (np.mean(cvauc), np.std(cvauc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"newData/image.npy\")\n",
    "Y = np.load(\"newData/label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict([X])\n",
    "\n",
    "y = label_binarize(Y, classes=[0, 1, 2])\n",
    "n_classes = y.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(num_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y[:, i], predict[:, i], )\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "roc_auc[\"macro\"] = roc_auc_score(y, predict, multi_class=\"ovo\", average=\"macro\")\n",
    "roc_auc[\"weighted\"] = roc_auc_score(y, predict, multi_class=\"ovo\", average=\"weighted\")\n",
    "\n",
    "\n",
    "y_pre = np.argmax(predict, axis=1)\n",
    "report = classification_report(Y, y_pre, output_dict=True)\n",
    "df1 = pd.DataFrame(report).transpose()\n",
    "df1['auc'] = [roc_auc[0], roc_auc[1], roc_auc[2], '', roc_auc[\"macro\"], roc_auc[\"weighted\"]]\n",
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
